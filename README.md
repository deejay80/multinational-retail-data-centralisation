# Multinational retail data centralization 

## Table of Contents

- [Description](#description)
- [Tools and Libraries](#tools-and-libraries)
- [Project Progress](#project-progress)
- [Installation](#installation)
- [Usage](#usage)
- [File Structure](#file-structure)
- [License](#license)

## Description

This project focuses on centralizing the sales data of our multinational company into a single database, enabling easy access and analysis. The primary goals include creating a unified data source, enhancing data-driven decision-making, and improving overall business intelligence.
## Tools
  ###  Technologies:
  - [PostgreSQl](#postgresql):  Robust relational database management system chosen for the project.
  - [AWS](#aws):  For cloud-based service and storage
  - [APIs](#apis):  Used for data retrieval
  ### Programming Language:
    Python
  ### Modules and Libraries:
 `Pandas`: Fast and flexible  data manipulation tool for analysis.


 `request`: Allows you to send HTTP request using Python.

 `tabula`: Enables the data extraction from PDF documents.

 `boto3`:  Amazon Web Services(AWS) software development kit(SDK) for python

 `re`:  Regular expression(or RE) can be used to check if a string contains a specified search pattern.  

 `logging`:Provide flexible framework  for emitting log messages from Python programs.

 `sqlalchemy`:  A toolkit for SQL interaction and an Object-Relational Mapping (ORM) library for database operations.

 `yaml`:  A library for working with YAML files.

### Project Progress

#### Week 1: Project Initiation

- Defined project goals and scope.
- Selected PostgreSQL as the DBMS for its robust features.
- Established initial database schema with tables for customers, products, and orders.

#### Week 2: ETL Process Implementation

- Identified and extracted sample sales data from CSV files and an API endpoint.
- Implemented data transformation steps to clean and standardize the extracted data.
- Completed the initial ETL process, loading data into the PostgreSQL database.

#### Week 3: Database Connection and Access

- Set up a secure connection to the centralized database.
- Developed a simple data access layer using SQL queries for basic interaction.

#### Week 4: Metrics and Monitoring

- Created SQL queries to retrieve key business metrics, including total revenue and top-selling products.
- Implemented monitoring tools to track database performance and health.
- Conducted initial testing and resolved performance bottlenecks.

### What I Learned

Throughout this project, I gained valuable experience in:

- Database design and schema creation.
- ETL process implementation for data transformation and loading.
- Establishing secure database connections and access controls.
- Querying databases for real-time business metrics.

## Usage
### Database design
The ERD follow a star-based schema as shown below:


![Alt text](sales_erd.drawio.png)

##  File structure
```
â””â”€â”€ ğŸ“ETL_scripts
       â””â”€â”€ database_utils.py
       â””â”€â”€ data_cleaning.py
       â””â”€â”€ data_extraction.py
â””â”€â”€ ğŸ“Excel_files
       â””â”€â”€ cleaned_card_data.csv
       â””â”€â”€ cleaned_data.csv
       â””â”€â”€ cleaned_date_data.csv
       â””â”€â”€ cleaned_orders_data.csv
       â””â”€â”€ cleaned_product_df.csv
       â””â”€â”€ cleaned_store_data.csv
       â””â”€â”€ cleaned_user_data.csv
       â””â”€â”€ date_data.csv
       â””â”€â”€ orders_data.csv
       â””â”€â”€ product_df.csv
       â””â”€â”€ stores_data.csv
       â””â”€â”€ user_data.csv

â””â”€â”€ ğŸ“SQL_scripts
       â””â”€â”€ m4_task1.sql
       â””â”€â”€ m4_task2.sql
       â””â”€â”€ m4_task3.sql
       â””â”€â”€ m4_task4.sql
       â””â”€â”€ m4_task5.sql
       â””â”€â”€ m4_task6.sql
       â””â”€â”€ m4_task7.sql
       â””â”€â”€ m4_task8.sql
       â””â”€â”€ m4_task9.sql
```
  

## Installation

